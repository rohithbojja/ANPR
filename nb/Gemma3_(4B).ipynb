{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohithbojja/ANPR/blob/main/nb/Gemma3_(4B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVRjr_2HXJpD"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjHWgGSkXJpF"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDEJqAjUXJpF"
      },
      "source": [
        "Read our **[TTS Guide](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** for instructions and all our notebooks.\n",
        "\n",
        "Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s-uXPO_XJpG"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b2sGyHUDXJpG"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# import os\n",
        "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "# else:\n",
        "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    # !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    # !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    # !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMWlrRdzwgf"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xbb0cuLzwgf",
        "outputId": "478f9bec-df37-48c0-ab74-0aa2e7b64d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-4b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a808c5-d516-4f00-ac0d-116f64b9181b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Gemma-3` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. Gemma-3 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<bos><start_of_turn>user\n",
        "Hello!<end_of_turn>\n",
        "<start_of_turn>model\n",
        "Hey there!<end_of_turn>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U dsad"
      ],
      "metadata": {
        "id": "AM_JF1kAXiLb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOJRLmJEYou_",
        "outputId": "3ec2c5a8-4e7c-41e1-a762-34af1008d9cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkq4RvEq7FQr",
        "outputId": "8dc50cc5-25eb-4596-b829-1034d0e6e329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'conversations': [{'from': 'human', 'value': 'summarize: ‡∞∏‡∞æ‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡∞ø‡∞ï ‡∞™‡±ã‡∞∑‡∞ï‡±Å‡∞°‡±Å ‡∞∏‡±å‡∞ú‡∞®‡±ç‡∞Ø ‡∞∂‡±Ä‡∞≤‡∞ø ‡∞µ‡±à‡∞é‡∞∏‡±ç‡∞∏‡∞æ‡∞Ü‡∞∞‡±ç ‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞ú‡∞®‡±ç‡∞Æ‡∞¶‡∞ø‡∞® ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ ‡∞∏‡∞æ‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡∞ø‡∞ï ‡∞∏‡∞Ç‡∞∏‡±ç‡∞•‡∞≤‡±Å ‡∞ú‡±Ä‡∞µ‡∞ø‡∞Ü‡∞∞‡±ç ‡∞Ü‡∞∞‡∞æ‡∞ß‡∞® ‡∞ï‡∞≤‡±ç‡∞ö‡∞∞‡∞≤‡±ç ‡∞´‡±å‡∞Ç‡∞°‡±á‡∞∑‡∞®‡±ç, ‡∞ï‡∞≥‡∞æ‡∞∞‡∞æ‡∞ú‡±ç‡∞Ø‡∞Ç, ‡∞∂‡∞ø‡∞ñ‡∞∞‡∞Ç ‡∞Ü‡∞∞‡±ç‡∞ü‡±ç‡∞∏‡±ç ‡∞•‡∞ø‡∞Ø‡±á‡∞ü‡∞∞‡±ç‡∞∏‡±ç, ‡∞µ‡±à‡∞ú‡±ç‡∞Æ‡±Ü‡∞®‡±ç ‡∞ï‡±ç‡∞≤‡∞¨‡±ç ‡∞Ü‡∞´‡±ç ‡∞∏‡∞ø‡∞ï‡∞ø‡∞Ç‡∞¶‡±ç‡∞∞‡∞æ‡∞¨‡∞æ‡∞¶‡±ç, ‡∞ï‡∞≥‡∞æ‡∞®‡∞ø‡∞≤‡∞Ø‡∞Ç ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞∂‡±ç‡∞∞‡±Ä‡∞§‡±ç‡∞Ø‡∞æ‡∞ó‡∞∞‡∞æ‡∞Ø ‡∞ó‡∞æ‡∞®‡∞∏‡∞≠‡∞≤‡±ã ‡∞®‡±á‡∞°‡±Å ‡∞∏‡∞§‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø ‡∞Ö‡∞§‡∞ø‡∞ß‡∞ø‡∞ó‡∞æ ‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ ‡∞∏‡∞≤‡∞π‡∞æ‡∞¶‡∞æ‡∞∞‡±Å ‡∞°‡∞æ. ‡∞ï‡±Ü‡∞µ‡∞ø ‡∞∞‡∞Æ‡∞£ ‡∞π‡∞æ‡∞ú‡∞∞‡±Å‡∞ï‡∞æ‡∞ó‡∞æ ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞ ‡∞π‡∞ï‡±ç‡∞ï‡±Å ‡∞ö‡∞ü‡±ç‡∞ü‡∞Ç ‡∞ï‡∞Æ‡∞ø‡∞∑‡∞®‡∞∞‡±ç ‡∞™‡∞ø. ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞¨‡∞æ‡∞¨‡±Å ‡∞∏‡∞æ‡∞π‡∞ø‡∞§‡±Ä‡∞µ‡±á‡∞§‡±ç‡∞§ ‡∞°‡∞æ. ‡∞™‡±ã‡∞§‡±Å‡∞ï‡±Ç‡∞∞‡±ç‡∞ö‡∞ø ‡∞∏‡∞æ‡∞Ç‡∞¨‡∞∂‡∞ø‡∞µ‡∞∞‡∞æ‡∞µ‡±Å ‡∞§‡∞¶‡∞ø‡∞§‡∞∞‡±Å‡∞≤‡±Å ‡∞™‡∞æ‡∞≤‡±ç‡∞ó‡±ä‡∞Ç‡∞ü‡∞æ‡∞∞‡±Å.'}, {'from': 'gpt', 'value': '‡∞∏‡∞æ‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡∞ø‡∞ï ‡∞™‡±ã‡∞∑‡∞ï‡±Å‡∞°‡±Å ‡∞∏‡±å‡∞ú‡∞®‡±ç‡∞Ø ‡∞∂‡±Ä‡∞≤‡∞ø ‡∞µ‡±à‡∞é‡∞∏‡±ç‡∞∏‡∞æ‡∞Ü‡∞∞‡±ç ‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞ú‡∞®‡±ç‡∞Æ‡∞¶‡∞ø‡∞® ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ ‡∞∏‡∞æ‡∞Ç‡∞∏‡±ç‡∞ï‡±É‡∞§‡∞ø‡∞ï ‡∞∏‡∞Ç‡∞∏‡±ç‡∞•‡∞≤‡±Å ‡∞∂‡±ç‡∞∞‡±Ä‡∞§‡±ç‡∞Ø‡∞æ‡∞ó‡∞∞‡∞æ‡∞Ø ‡∞ó‡∞æ‡∞®‡∞∏‡∞≠‡∞≤‡±ã ‡∞®‡±á‡∞°‡±Å ‡∞∏‡∞§‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø. ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø ‡∞Ö‡∞§‡∞ø‡∞ß‡∞ø ‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞™‡±ç‡∞∞‡∞Æ‡±Å‡∞ñ ‡∞∏‡∞≤‡∞π‡∞æ‡∞¶‡∞æ‡∞∞‡±Å ‡∞°‡∞æ. ‡∞ï‡±Ü‡∞µ‡∞ø ‡∞∞‡∞Æ‡∞£ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞§‡∞¶‡∞ø‡∞§‡∞∞‡±Å‡∞≤‡±Å ‡∞™‡∞æ‡∞≤‡±ç‡∞ó‡±ä‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å.'}]}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the original dataset\n",
        "dataset = load_dataset(\"rbojja/telugu_news_summarization\")\n",
        "\n",
        "# Transformation function\n",
        "def convert_to_chat(example):\n",
        "    return {\n",
        "        \"conversations\": [\n",
        "            {\n",
        "                \"from\": \"human\",\n",
        "                \"value\": f\"summarize: {example['text']}\"\n",
        "            },\n",
        "            {\n",
        "                \"from\": \"gpt\",\n",
        "                \"value\": example[\"summary\"]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Apply the transformation to each split\n",
        "chat_dataset = DatasetDict()\n",
        "for split in dataset.keys():\n",
        "    chat_dataset[split] = dataset[split].map(\n",
        "        convert_to_chat,\n",
        "        remove_columns=dataset[split].column_names\n",
        "    )\n",
        "\n",
        "# Sanity check\n",
        "print(chat_dataset[\"train\"][0])\n",
        "dataset = chat_dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_data_formats` to try converting datasets to the correct format for finetuning purposes!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6_BKie_X5KH",
        "outputId": "b064c890-efbe-412a-bb50-92bb9702a21c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': [{'from': 'human',\n",
              "   'value': 'summarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.'},\n",
              "  {'from': 'gpt',\n",
              "   'value': '‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Ä ‡∞µ‡∞ø‡∞ú‡±ç‡∞û‡∞™‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Ç ,‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞ö‡±á‡∞∏‡∞æ‡∞∞‡±Å .'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "reoBXmAn7HlN"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import standardize_data_formats\n",
        "dataset = standardize_data_formats(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i5Sx9In7vHi"
      },
      "source": [
        "Let's see how row 100 looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzE1OEXi7s3P",
        "outputId": "b70e3986-75d3-41a5-fa9d-edc007bab4a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'conversations': [{'content': 'summarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.',\n",
              "   'role': 'user'},\n",
              "  {'content': '‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Ä ‡∞µ‡∞ø‡∞ú‡±ç‡∞û‡∞™‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Ç ,‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞ö‡±á‡∞∏‡∞æ‡∞∞‡±Å .',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xs0LXio7rfd"
      },
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1ahE8Ys37JDJ"
      },
      "outputs": [],
      "source": [
        "def apply_chat_template(examples):\n",
        "    texts = tokenizer.apply_chat_template(examples[\"conversations\"])\n",
        "    return { \"text\" : texts }\n",
        "pass\n",
        "dataset = dataset.map(apply_chat_template, batched = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "Let's see how the chat template did! Notice `Gemma-3` default adds a `<bos>`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "gGFzmplrEy9I",
        "outputId": "77a87efd-ad60-4ca7-b8ef-bfb08f75b4a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nsummarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.<end_of_turn>\\n<start_of_turn>model\\n‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Ä ‡∞µ‡∞ø‡∞ú‡±ç‡∞û‡∞™‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Ç ,‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞ö‡±á‡∞∏‡∞æ‡∞∞‡±Å .<end_of_turn>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataset[100][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "82c74d8c-041e-4452-890b-a6f8aefe63eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 32,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "LtsMVtlkUhja",
        "outputId": "ae18fab4-1eaf-489d-af98-d2f448e4a7d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><bos><start_of_turn>user\\nsummarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.<end_of_turn>\\n<start_of_turn>model\\n‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Ä ‡∞µ‡∞ø‡∞ú‡±ç‡∞û‡∞™‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Ç ,‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞ö‡±á‡∞∏‡∞æ‡∞∞‡±Å .<end_of_turn>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kyjy__m9KY3"
      },
      "source": [
        "Now let's print the masked out example - you should see only the answer is present:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "92a0b83b-2505-4b2c-ebf4-9d214f4b6876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                        ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡±Ä ‡∞µ‡∞ø‡∞ú‡±ç‡∞û‡∞™‡±ç‡∞§‡∞ø ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±Ç ,‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞ö‡±á‡∞∏‡∞æ‡∞∞‡±Å .<end_of_turn>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "3f38ba3d-3473-4c8d-8bb2-05a9d587fc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.57 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNP1Uidk9mrz"
      },
      "source": [
        "Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "c34aeffc-483f-49a7-e5cf-ff11e20ac997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 16,295 | Num Epochs = 1 | Total steps = 127\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 4 x 1) = 128\n",
            " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 96.12 MiB is free. Process 111465 has 14.64 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 230.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3d62c575fcfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         outputs = super().compute_loss(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         )\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_compute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1758\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/temporary_patches/gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         outputs = self.language_model(\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     ) -> CausalLMOutputWithPast:\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGemma3ForCausalLM_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     def prepare_inputs_for_generation(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\u001b[0m in \u001b[0;36mGemma3ForCausalLM_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m     \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, last_cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    709\u001b[0m                     \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflash_attn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/gradient_checkpointing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_gpu_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMAIN_STREAM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXTRA_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3/modeling_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings_global, position_embeddings_local, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, last_cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/temporary_patches/gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m#     **kwargs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         attn_output = scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    649\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdowncast_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdowncast_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 190.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 96.12 MiB is free. Process 111465 has 14.64 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 230.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\n",
        "        \"type\" : \"text\",\n",
        "        \"text\" : \"summarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.\",\n",
        "    }]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "outputs = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        ")\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"summarize: ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞® ‡∞ú‡±Ä‡∞µ‡±ã‡∞®‡±Å ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞æ‡∞≤‡∞®‡∞ø ‡∞ï‡±ã‡∞∞‡±Å‡∞§‡±Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞æ‡∞° ‡∞ï‡±É‡∞∑‡±ç‡∞£‡∞≤‡∞Ç‡∞ï‡∞≤‡±ã‡∞®‡∞ø ‡∞∞‡∞Ç‡∞ó‡∞æ ‡∞π‡±à‡∞∏‡±ç‡∞ï‡±Ç‡∞≤‡±ç ‡∞µ‡∞¶‡±ç‡∞¶ ‡∞Æ‡±Å‡∞®‡±ç‡∞∏‡∞ø‡∞™‡∞≤‡±ç ‡∞ï‡∞æ‡∞∞‡±ç‡∞™‡±ä‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞µ‡∞∞‡±ç‡∞ï‡∞∞‡±ç‡∞∏‡±ç ‡∞Ø‡±Ç‡∞®‡∞ø‡∞Ø‡∞®‡±ç ( ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ) ‡∞Ü‡∞ß‡±ç‡∞µ‡∞∞‡±ç‡∞Ø‡∞Ç‡∞≤‡±ã ‡∞¨‡±Å‡∞ß ‡∞µ‡∞æ‡∞∞‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡±Å ‡∞ß‡∞∞‡±ç‡∞®‡∞æ ‡∞®‡∞ø‡∞∞‡±ç‡∞µ‡∞π‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å. ‡∞à ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞ó‡∞æ ‡∞∏‡∞ø‡∞ê‡∞ü‡∞ø‡∞Ø‡±Å ‡∞§‡±Ç‡∞∞‡±ç‡∞™‡±Å ‡∞ú‡±ã‡∞®‡±ç 1 ‡∞ï‡∞æ‡∞∞‡±ç‡∞Ø‡∞¶‡∞∞‡±ç‡∞∂‡∞ø ‡∞µ‡∞ø. ‡∞ó‡±Å‡∞∞‡±Å‡∞Æ‡±Ç‡∞∞‡±ç‡∞§‡∞ø ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡±Å‡∞§‡±Ç ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞ ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞™‡∞æ‡∞∞‡∞ø‡∞∂‡±Å‡∞ß‡±ç‡∞Ø ‡∞ï‡∞æ‡∞∞‡±ç‡∞Æ‡∞ø‡∞ï‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±Ü‡∞Ç‡∞ö‡∞ø‡∞® ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞≤‡±Å ‡∞Æ‡∞Ç‡∞ú‡±Ç‡∞∞‡±Å ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞°‡∞æ ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡±ã‡∞Ç‡∞¶‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å. ‡∞∏‡∞Æ‡±ç‡∞Æ‡±Ü ‡∞ï‡∞æ‡∞≤‡∞™‡±Å ‡∞µ‡±á‡∞§‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞á‡∞Ç‡∞§‡∞µ‡∞∞‡∞ï‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞≤‡±á‡∞¶‡∞®‡∞ø ‡∞§‡±Ü‡∞≤‡∞ø‡∞™‡∞æ‡∞∞‡±Å. ‡∞™‡±ç‡∞∞‡∞≠‡±Å‡∞§‡±ç‡∞µ‡∞Ç ‡∞µ‡±Ü‡∞Ç‡∞ü‡∞®‡±á ‡∞ú‡±Ä‡∞µ‡±ã ‡∞µ‡∞ø‡∞°‡±Å‡∞¶‡∞≤ ‡∞ö‡±á‡∞Ø‡∞ï‡±Å‡∞Ç‡∞ü‡±á ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞® ‡∞§‡±Ä‡∞µ‡±ç‡∞∞‡∞§‡∞∞‡∞Ç ‡∞ö‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞Æ‡∞®‡∞ø ‡∞π‡±Ü‡∞ö‡±ç‡∞ö‡∞∞‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å.\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")  # Local saving\n",
        "tokenizer.save_pretrained(\"gemma-3\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastModel\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n",
        "\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save finetune!\n",
        "    model.save_pretrained_merged(\"gemma-3-finetune\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O48DbNIAr0"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-CiKPrIFG0"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to save to GGUF\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q974YEVPI7JS"
      },
      "source": [
        "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgcJIhJ0I_es"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload GGUF\n",
        "    model.push_to_hub_gguf(\n",
        "        \"gemma-3-finetune\",\n",
        "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
        "        repo_id = \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
        "        token = \"hf_...\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gV_2nKzXJpV"
      },
      "source": [
        "Now, use the `gemma-3-finetune.gguf` file or `gemma-3-finetune-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(\"rbojja/telugu_news_summarization\")\n",
        "test_ds = ds[\"test\"]\n",
        "\n",
        "# Models to compare\n",
        "model_names = [\n",
        "    \"rbojja/mt5-summarize-te_v3\",\n",
        "    \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "]\n",
        "\n",
        "# Prepare ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def generate_summaries(model_name, test_ds, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", batch_size=96):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    preds, refs = [], []\n",
        "    for i in tqdm(range(0, len(test_ds), batch_size), desc=f\"Generating with {model_name}\"):\n",
        "        batch = test_ds[i:i+batch_size]\n",
        "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=4,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        preds.extend(decoded_preds)\n",
        "        refs.extend(batch[\"summary\"])\n",
        "    return preds, refs\n",
        "\n",
        "results = {}\n",
        "for model_name in model_names:\n",
        "    preds, refs = generate_summaries(model_name, test_ds)\n",
        "    # Add period if missing (for ROUGE)\n",
        "    preds = [p if p.endswith((\".\", \"!\", \"?\", \"‡•§\", \"‡••\")) else p + \".\" for p in preds]\n",
        "    refs = [r if r.endswith((\".\", \"!\", \"?\", \"‡•§\", \"‡••\")) else r + \".\" for r in refs]\n",
        "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
        "    results[model_name] = rouge_scores\n",
        "\n",
        "# Print results\n",
        "for model_name, scores in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    for k, v in scores.items():\n",
        "        print(f\"{k}: {v:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "3fe017377705436f84507020e410ad94",
            "ee086c9c962543cbbd87da37d2c1731e",
            "32f84f7a0c1748748baa6ce56aa9ac04",
            "7e3c3c1c3b004baf918b691e873f14c1",
            "24e54e937e834b7a836830d8428fbc87",
            "311e73ed26704877bf231f9e68d45e79",
            "9cc84b41c54b473aae9fef8ca1dc88e8",
            "c4dfbe1805ad41a295df35432e150e74",
            "5bf5525aac2d4df3965beafe868c8ae4",
            "f3824ef365e347049cda1625d49cc9aa",
            "e1a6c6ad75ff42489a85f8b5032d65cf",
            "9c41c2b5262d40e9a6643cd96107bf81",
            "dff8f002106f47f08cfffeb67b436dce",
            "025e3cbd78314ebfa2633510dc55b9f9",
            "05c3434610b54cb9ac8a06deebac4c8a",
            "c9bb3fb3806c4a01baa525728af87c63",
            "71141dcc059940aab9fccc5056c532b8",
            "4c37ae10222749c2ad65c0d30d488cd6",
            "0a8bde7025c141fea9c2714c2efb25c1",
            "67981e24f192450f90358ee050360980",
            "f009bca0fff6468da7a0cbbf433bd580",
            "61a991d4855c46a4832790f16dd5cec6",
            "bafb13efb0454f369fe045ffe4f3e328",
            "732d71c7f1524dc59a7b69d8e54a6f7b",
            "954d118ea71246e58cf0fb282dc9bb46",
            "5b9345b1fa124710a86dc815e07db22b",
            "be59b45636f1462bad6a20562cf0530c",
            "8a96d4ef045543e0806de2efa86ed888",
            "831908df158f458097e0572982869d20",
            "ba7a49a8d2b840dc97e08875efc27b13",
            "8b6c5108fe7846ed8444adb3f006c066",
            "199e65aeee06423fb2db8718c1245cfb",
            "23c60a17e21e4ae39961864c36433b8d",
            "a7f163f7d7e24050827de4d41c646d18",
            "8ba0071b130443b58e71a651bcd6518f",
            "1824966502974153950a78cb22662a42",
            "35d0492bad4940baa89dc7e7b456f4ef",
            "76cf42d514a642359feef0d67565a6c0",
            "1ed09c37fdd444238a7e8b4da55bd940",
            "982808df44f446678c585d4f64c0ae4c",
            "805e477f0ab44daeb88ed3d104880802",
            "b5c392cd9a29497cbaf8b9fde9c0704d",
            "1eaa7f74b7364149a2aacbb608975a05",
            "03a9d2586e4642bda46d33d8b1aa8ed8",
            "125d702d77fb488d8862f0ab641a6de4",
            "face66911d6440dd86483c0a8be99993",
            "b646ec063dfa4317a06a2e4e500ffc51",
            "aa10fc94d92c413a91a554ad13e9cffe",
            "7d146af70e7749559cf991896aeeb0b5",
            "5c5f168e7e4d47899e00acd0b03adc0f",
            "ca0e1058d3d644d4aab81439144c28d6",
            "081f58103a9a4971aa0f7853cba9b9e4",
            "d13ab0d45d754fe7a36a94f8dd64cc1f",
            "364420cc73294434bd3cf8f4ac8af9af",
            "30e90a803133420c98e54b89d306bff3",
            "b01ac09a1c9f48e2a5215d7f8ebf73b6",
            "31e87b7987134d7ba19542759bc727a0",
            "3a93379d0fb34a05ac24e487fb23dd31",
            "7e168857641b470594af4fc34de8834d",
            "9fe668ddb7864ddebaaae70eb88f8442",
            "9b2c814714bc44728c10f4481dfaafcf",
            "e4b07765343f4dc486c89f9142ba7f64",
            "6c4b17917d484694a84bba9f6eee5484",
            "0376cb413ffc4ac7a3a06f6c2ea0278b",
            "fc44978406684094bf361ea404437156",
            "6dc0291e510e4b1798b93f318b814bce"
          ]
        },
        "id": "SuaEvFR6lyEO",
        "outputId": "b4918821-90d6-4cad-d9da-237d8c902bcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/19.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fe017377705436f84507020e410ad94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c41c2b5262d40e9a6643cd96107bf81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bafb13efb0454f369fe045ffe4f3e328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/416 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7f163f7d7e24050827de4d41c646d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/946 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "125d702d77fb488d8862f0ab641a6de4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b01ac09a1c9f48e2a5215d7f8ebf73b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating with rbojja/mt5-summarize-te_v3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [12:45<00:00, 34.80s/it]\n",
            "Generating with csebuetnlp/mT5_multilingual_XLSum: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [07:02<00:00, 19.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model: rbojja/mt5-summarize-te_v3\n",
            "rouge1: 0.1980\n",
            "rouge2: 0.0500\n",
            "rougeL: 0.1937\n",
            "rougeLsum: 0.1944\n",
            "\n",
            "Model: csebuetnlp/mT5_multilingual_XLSum\n",
            "rouge1: 0.0563\n",
            "rouge2: 0.0028\n",
            "rougeL: 0.0560\n",
            "rougeLsum: 0.0561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import evaluate\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(\"rbojja/telugu_news_summarization\")\n",
        "test_ds = ds[\"test\"]\n",
        "\n",
        "# Models to compare\n",
        "model_names = [\n",
        "    \"rbojja/mt5-summarize-te_v3\",\n",
        "    \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "    \"rbojja/mt5-summarize-te_v2\"\n",
        "]\n",
        "\n",
        "def generate_summaries(model_name, test_ds, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", batch_size=32):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    preds = []\n",
        "    for i in tqdm(range(0, len(test_ds), batch_size), desc=f\"Generating with {model_name}\"):\n",
        "        batch = test_ds[i:i+batch_size]\n",
        "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=128,\n",
        "                num_beams=4,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        preds.extend(decoded_preds)\n",
        "    return preds\n",
        "\n",
        "# Generate and save after first model\n",
        "first_model = model_names[0]\n",
        "first_preds = generate_summaries(first_model, test_ds)\n",
        "results = {\n",
        "    \"text\": test_ds[\"text\"],\n",
        "    \"ground_truth\": test_ds[\"summary\"],\n",
        "    \"mt5_summarize_te_v3\": first_preds\n",
        "}\n",
        "results_ds = Dataset.from_dict(results)\n",
        "results_ds.save_to_disk(\"telugu_news_summarization_model_comparison\")\n",
        "print(f\"Saved predictions for {first_model}\")\n",
        "\n",
        "# Generate and save after second model\n",
        "second_model = model_names[1]\n",
        "second_preds = generate_summaries(second_model, test_ds)\n",
        "# Load the previously saved dataset and add new column\n",
        "results_ds = load_from_disk(\"telugu_news_summarization_model_comparison\")\n",
        "results_ds = results_ds.add_column(\"mt5_multilingual_xlsum\", second_preds)\n",
        "results_ds.save_to_disk(\"telugu_news_summarization_model_comparison\")\n",
        "print(f\"Saved predictions for {second_model}\")\n",
        "\n",
        "# Generate and save after third model\n",
        "third_model = model_names[1]\n",
        "third_preds = generate_summaries(third_model, test_ds)\n",
        "# Load the previously saved dataset and add new column\n",
        "results_ds = load_from_disk(\"telugu_news_summarization_model_comparison\")\n",
        "results_ds = results_ds.add_column(\"telugu_news_summarization_model_comparison_v3\", third_preds)\n",
        "results_ds.save_to_disk(\"telugu_news_summarization_model_comparison\")\n",
        "print(f\"Saved predictions for {second_model}\")\n",
        "\n",
        "# Calculate BLEU scores for each model\n",
        "import evaluate\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "references = [[ref] for ref in test_ds[\"summary\"]]\n",
        "bleu_score_1 = bleu.compute(predictions=first_preds, references=references)\n",
        "bleu_score_2 = bleu.compute(predictions=second_preds, references=references)\n",
        "bleu_score_3 = bleu.compute(predictions=third_preds, references=references)\n",
        "\n",
        "print(f\"{first_model} BLEU: {bleu_score_1['bleu']:.4f}\")\n",
        "print(f\"{second_model} BLEU: {bleu_score_2['bleu']:.4f}\")\n",
        "print(f\"{third_model} BLEU: {bleu_score_3['bleu']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpYn9Xyy06Lj",
        "outputId": "61dd98e9-d619-439f-ff70-e55cd46dbc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating with rbojja/mt5-summarize-te_v3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/64 [12:30<10:36, 22.72s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fe017377705436f84507020e410ad94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee086c9c962543cbbd87da37d2c1731e",
              "IPY_MODEL_32f84f7a0c1748748baa6ce56aa9ac04",
              "IPY_MODEL_7e3c3c1c3b004baf918b691e873f14c1"
            ],
            "layout": "IPY_MODEL_24e54e937e834b7a836830d8428fbc87"
          }
        },
        "ee086c9c962543cbbd87da37d2c1731e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311e73ed26704877bf231f9e68d45e79",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9cc84b41c54b473aae9fef8ca1dc88e8",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "32f84f7a0c1748748baa6ce56aa9ac04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4dfbe1805ad41a295df35432e150e74",
            "max": 19282,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bf5525aac2d4df3965beafe868c8ae4",
            "value": 19282
          }
        },
        "7e3c3c1c3b004baf918b691e873f14c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3824ef365e347049cda1625d49cc9aa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e1a6c6ad75ff42489a85f8b5032d65cf",
            "value": "‚Äá19.3k/19.3k‚Äá[00:00&lt;00:00,‚Äá1.21MB/s]"
          }
        },
        "24e54e937e834b7a836830d8428fbc87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "311e73ed26704877bf231f9e68d45e79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc84b41c54b473aae9fef8ca1dc88e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4dfbe1805ad41a295df35432e150e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf5525aac2d4df3965beafe868c8ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3824ef365e347049cda1625d49cc9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a6c6ad75ff42489a85f8b5032d65cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c41c2b5262d40e9a6643cd96107bf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dff8f002106f47f08cfffeb67b436dce",
              "IPY_MODEL_025e3cbd78314ebfa2633510dc55b9f9",
              "IPY_MODEL_05c3434610b54cb9ac8a06deebac4c8a"
            ],
            "layout": "IPY_MODEL_c9bb3fb3806c4a01baa525728af87c63"
          }
        },
        "dff8f002106f47f08cfffeb67b436dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71141dcc059940aab9fccc5056c532b8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4c37ae10222749c2ad65c0d30d488cd6",
            "value": "spiece.model:‚Äá100%"
          }
        },
        "025e3cbd78314ebfa2633510dc55b9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a8bde7025c141fea9c2714c2efb25c1",
            "max": 4309802,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67981e24f192450f90358ee050360980",
            "value": 4309802
          }
        },
        "05c3434610b54cb9ac8a06deebac4c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f009bca0fff6468da7a0cbbf433bd580",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61a991d4855c46a4832790f16dd5cec6",
            "value": "‚Äá4.31M/4.31M‚Äá[00:00&lt;00:00,‚Äá71.9MB/s]"
          }
        },
        "c9bb3fb3806c4a01baa525728af87c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71141dcc059940aab9fccc5056c532b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c37ae10222749c2ad65c0d30d488cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a8bde7025c141fea9c2714c2efb25c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67981e24f192450f90358ee050360980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f009bca0fff6468da7a0cbbf433bd580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a991d4855c46a4832790f16dd5cec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bafb13efb0454f369fe045ffe4f3e328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_732d71c7f1524dc59a7b69d8e54a6f7b",
              "IPY_MODEL_954d118ea71246e58cf0fb282dc9bb46",
              "IPY_MODEL_5b9345b1fa124710a86dc815e07db22b"
            ],
            "layout": "IPY_MODEL_be59b45636f1462bad6a20562cf0530c"
          }
        },
        "732d71c7f1524dc59a7b69d8e54a6f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a96d4ef045543e0806de2efa86ed888",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_831908df158f458097e0572982869d20",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "954d118ea71246e58cf0fb282dc9bb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba7a49a8d2b840dc97e08875efc27b13",
            "max": 16349930,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b6c5108fe7846ed8444adb3f006c066",
            "value": 16349930
          }
        },
        "5b9345b1fa124710a86dc815e07db22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_199e65aeee06423fb2db8718c1245cfb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23c60a17e21e4ae39961864c36433b8d",
            "value": "‚Äá16.3M/16.3M‚Äá[00:00&lt;00:00,‚Äá18.9MB/s]"
          }
        },
        "be59b45636f1462bad6a20562cf0530c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a96d4ef045543e0806de2efa86ed888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831908df158f458097e0572982869d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba7a49a8d2b840dc97e08875efc27b13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6c5108fe7846ed8444adb3f006c066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "199e65aeee06423fb2db8718c1245cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c60a17e21e4ae39961864c36433b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7f163f7d7e24050827de4d41c646d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ba0071b130443b58e71a651bcd6518f",
              "IPY_MODEL_1824966502974153950a78cb22662a42",
              "IPY_MODEL_35d0492bad4940baa89dc7e7b456f4ef"
            ],
            "layout": "IPY_MODEL_76cf42d514a642359feef0d67565a6c0"
          }
        },
        "8ba0071b130443b58e71a651bcd6518f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ed09c37fdd444238a7e8b4da55bd940",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_982808df44f446678c585d4f64c0ae4c",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "1824966502974153950a78cb22662a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805e477f0ab44daeb88ed3d104880802",
            "max": 416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5c392cd9a29497cbaf8b9fde9c0704d",
            "value": 416
          }
        },
        "35d0492bad4940baa89dc7e7b456f4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eaa7f74b7364149a2aacbb608975a05",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_03a9d2586e4642bda46d33d8b1aa8ed8",
            "value": "‚Äá416/416‚Äá[00:00&lt;00:00,‚Äá38.8kB/s]"
          }
        },
        "76cf42d514a642359feef0d67565a6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed09c37fdd444238a7e8b4da55bd940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982808df44f446678c585d4f64c0ae4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "805e477f0ab44daeb88ed3d104880802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c392cd9a29497cbaf8b9fde9c0704d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1eaa7f74b7364149a2aacbb608975a05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a9d2586e4642bda46d33d8b1aa8ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "125d702d77fb488d8862f0ab641a6de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_face66911d6440dd86483c0a8be99993",
              "IPY_MODEL_b646ec063dfa4317a06a2e4e500ffc51",
              "IPY_MODEL_aa10fc94d92c413a91a554ad13e9cffe"
            ],
            "layout": "IPY_MODEL_7d146af70e7749559cf991896aeeb0b5"
          }
        },
        "face66911d6440dd86483c0a8be99993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c5f168e7e4d47899e00acd0b03adc0f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ca0e1058d3d644d4aab81439144c28d6",
            "value": "config.json:‚Äá100%"
          }
        },
        "b646ec063dfa4317a06a2e4e500ffc51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_081f58103a9a4971aa0f7853cba9b9e4",
            "max": 946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d13ab0d45d754fe7a36a94f8dd64cc1f",
            "value": 946
          }
        },
        "aa10fc94d92c413a91a554ad13e9cffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_364420cc73294434bd3cf8f4ac8af9af",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30e90a803133420c98e54b89d306bff3",
            "value": "‚Äá946/946‚Äá[00:00&lt;00:00,‚Äá93.6kB/s]"
          }
        },
        "7d146af70e7749559cf991896aeeb0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5f168e7e4d47899e00acd0b03adc0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0e1058d3d644d4aab81439144c28d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "081f58103a9a4971aa0f7853cba9b9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13ab0d45d754fe7a36a94f8dd64cc1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "364420cc73294434bd3cf8f4ac8af9af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e90a803133420c98e54b89d306bff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b01ac09a1c9f48e2a5215d7f8ebf73b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31e87b7987134d7ba19542759bc727a0",
              "IPY_MODEL_3a93379d0fb34a05ac24e487fb23dd31",
              "IPY_MODEL_7e168857641b470594af4fc34de8834d"
            ],
            "layout": "IPY_MODEL_9fe668ddb7864ddebaaae70eb88f8442"
          }
        },
        "31e87b7987134d7ba19542759bc727a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b2c814714bc44728c10f4481dfaafcf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4b07765343f4dc486c89f9142ba7f64",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "3a93379d0fb34a05ac24e487fb23dd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c4b17917d484694a84bba9f6eee5484",
            "max": 2329638768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0376cb413ffc4ac7a3a06f6c2ea0278b",
            "value": 2329638768
          }
        },
        "7e168857641b470594af4fc34de8834d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc44978406684094bf361ea404437156",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6dc0291e510e4b1798b93f318b814bce",
            "value": "‚Äá2.33G/2.33G‚Äá[01:39&lt;00:00,‚Äá23.3MB/s]"
          }
        },
        "9fe668ddb7864ddebaaae70eb88f8442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2c814714bc44728c10f4481dfaafcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b07765343f4dc486c89f9142ba7f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c4b17917d484694a84bba9f6eee5484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0376cb413ffc4ac7a3a06f6c2ea0278b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc44978406684094bf361ea404437156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc0291e510e4b1798b93f318b814bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}